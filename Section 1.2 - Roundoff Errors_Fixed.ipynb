{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.2: Round-off Errors and Computer Arithmetic\n",
    "\n",
    "## Floating-point arithmetic\n",
    "\n",
    "Real numbers are stored on a computer following the IEEE floating-point standard:\n",
    "\n",
    "1. **half precision** using 16 bits (Julia type: `Float16`)\n",
    "2. **single precision** using 32 bits (Julia type: `Float32`)\n",
    "3. **double precision** using 64 bits (Julia type: `Float64`)\n",
    "\n",
    "Julia also has an **arbitrary precision** floating-point data type called `BigFloat`. It is excellent if you need more **precision**, but it is also much **slower**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of IEEE double floating-point format (`Float64`)\n",
    "\n",
    "Suppose $x$ is a floating-point number stored in the following 64-bits:\n",
    "\n",
    "| 1 | 2 | $\\cdots$ | 12 | 13 | $\\cdots$ | 64 |\n",
    "|:-:|:-:|:--------:|:--:|:--:|:--------:|:--:|\n",
    "|$s$|$e_{10}$| $\\cdots$ |$e_0$|$f_1$|$\\cdots$|$f_{52}$|\n",
    "\n",
    "- 1 bit $s$ represents the **sign**\n",
    "- 11 bits $e_{10} \\cdots e_{0}$ represent the **exponent**\n",
    "- 52 bits $f_1 \\cdots f_{52}$ represent the **fraction** (a.k.a. the mantissa or significand)\n",
    "\n",
    "Then\n",
    "\n",
    "$$ x = (-1)^s \\left[1.f_1 \\cdots f_{52}\\right]_2 \\times 2^{(e-1023)}.$$\n",
    "\n",
    "Notes: \n",
    "\n",
    "- $x$ is **normalized** to have its first digit nonzero.\n",
    "- $$e = \\left[e_{10} \\cdots e_{0}\\right]_2 = e_{10} 2^{10} + \\cdots + e_1 2^1 + e_0 2^0 \\in \\left[0, 2^{11}-1\\right] = [0, 2047]$$\n",
    "- $e = 0$ and $e = 2047$ are reserved for special floating-point values, so \n",
    "\n",
    "$$e \\in [1, 2046]$$\n",
    "\n",
    "- the \"$-1023$\" in the exponent is called the **bias**:  $e-1023 \\in [-1022,1023]$ (**In the book is considered the bias exponent in $[-1023,1024]$**)\n",
    "- $$\\left[1.f_1 \\cdots f_{52}\\right]_2 = 1 + \\frac{f_1}{2^1} + \\frac{f_2}{2^2} + \\cdots + \\frac{f_{52}}{2^{52}}$$\n",
    "in base $2$.\n",
    "\n",
    "![title](Mantissa2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "x & = -[1.101101]_2 \\times 2^{(1026-1023)} \\\\\n",
    "  & = -[1.101101]_2 \\times 2^{3} \\\\\n",
    "  & = -[1101.101]_2 \\\\\n",
    "  & = -\\left(1 \\cdot 8 + 1 \\cdot 4 + 0 \\cdot 2 + 1 \\cdot 2^0 + 1 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{8}\\right)  \\\\\n",
    "  & = -13.625\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[1mb\u001b[22m\u001b[1mi\u001b[22m\u001b[1mt\u001b[22m\u001b[1ms\u001b[22m is\u001b[1mb\u001b[22m\u001b[1mi\u001b[22m\u001b[1mt\u001b[22m\u001b[1ms\u001b[22m flip\u001b[1mb\u001b[22m\u001b[1mi\u001b[22m\u001b[1mt\u001b[22m\u001b[1ms\u001b[22m! \u001b[1mb\u001b[22m\u001b[1mi\u001b[22m\u001b[1mt\u001b[22mbroadca\u001b[1ms\u001b[22mt \u001b[1mb\u001b[22m\u001b[1mi\u001b[22m\u001b[1mt\u001b[22mrand \u001b[1mB\u001b[22m\u001b[1mi\u001b[22m\u001b[1mt\u001b[22mArray \u001b[1mB\u001b[22m\u001b[1mi\u001b[22m\u001b[1mt\u001b[22mVector \u001b[1mB\u001b[22m\u001b[1mi\u001b[22m\u001b[1mt\u001b[22mMatrix\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "bits(n)\n",
       "```\n",
       "\n",
       "A string giving the literal bit representation of a number.\n",
       "\n",
       "```jldoctest\n",
       "julia> bits(4)\n",
       "\"0000000000000000000000000000000000000000000000000000000000000100\"\n",
       "\n",
       "julia> bits(2.2)\n",
       "\"0100000000000001100110011001100110011001100110011001100110011010\"\n",
       "```\n"
      ],
      "text/plain": [
       "```\n",
       "bits(n)\n",
       "```\n",
       "\n",
       "A string giving the literal bit representation of a number.\n",
       "\n",
       "```jldoctest\n",
       "julia> bits(4)\n",
       "\"0000000000000000000000000000000000000000000000000000000000000100\"\n",
       "\n",
       "julia> bits(2.2)\n",
       "\"0100000000000001100110011001100110011001100110011001100110011010\"\n",
       "```\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1', \"10000000010\", \"1011010000000000000000000000000000000000000000000000\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = bits(-13.625)\n",
    "s[1], s[2:12], s[13:64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Even if a number can be represented exactly in base-10 with a finite number of digits, it may require an infinite number of digits in base-2.\n",
    "\n",
    "$$\n",
    "0.1 = \\left[0.000110011001\\ldots\\right]_2 = \\left[1.\\overline{1001}\\right]_2 \\times 2^{-4}\n",
    "$$\n",
    "\n",
    "Therefore, $0.1$ cannot be represented exactly as a floating-point number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0',\"01111111011\",\"1001100110011001100110011001100110011001100110011010\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = bits(0.1)\n",
    "s[1], s[2:12], s[13:64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limits of floating-point numbers\n",
    "\n",
    "- **Largest** `Float64` $= \\left(2 - 2^{-52}\\right) \\times 2^{1023} \\approx 2 \\times 10^{308}$\n",
    "- **Smallest positive normalized** `Float64` $= 2^{-1022} \\approx 2 \\times 10^{-308}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[1mr\u001b[22m\u001b[1me\u001b[22m\u001b[1ma\u001b[22m\u001b[1ml\u001b[22m\u001b[1mm\u001b[22m\u001b[1ma\u001b[22m\u001b[1mx\u001b[22m \u001b[1mr\u001b[22m\u001b[1me\u001b[22m\u001b[1ma\u001b[22m\u001b[1ml\u001b[22m\u001b[1mm\u001b[22min \u001b[1mr\u001b[22m\u001b[1me\u001b[22m\u001b[1ma\u001b[22mdd\u001b[1ml\u001b[22m\u001b[1mm\u001b[22m \u001b[1mR\u001b[22m\u001b[1me\u001b[22m\u001b[1ma\u001b[22mdOn\u001b[1ml\u001b[22my\u001b[1mM\u001b[22memoryError\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "realmax(T)\n",
       "```\n",
       "\n",
       "The highest finite value representable by the given floating-point DataType `T`.\n",
       "\n",
       "```jldoctest\n",
       "julia> realmax(Float16)\n",
       "Float16(6.55e4)\n",
       "\n",
       "julia> realmax(Float32)\n",
       "3.4028235f38\n",
       "```\n"
      ],
      "text/plain": [
       "```\n",
       "realmax(T)\n",
       "```\n",
       "\n",
       "The highest finite value representable by the given floating-point DataType `T`.\n",
       "\n",
       "```jldoctest\n",
       "julia> realmax(Float16)\n",
       "Float16(6.55e4)\n",
       "\n",
       "julia> realmax(Float32)\n",
       "3.4028235f38\n",
       "```\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?realmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7976931348623157e308"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with realmax\n",
    "x = realmax(Float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0',\"11111111110\",\"1111111111111111111111111111111111111111111111111111\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = bits(x)\n",
    "s[1], s[2:12], s[13:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inf"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x + 1) - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mn\u001b[22m \u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mm\u001b[22max\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "floatmin(T)\n",
       "\\end{verbatim}\n",
       "The smallest in absolute value non-subnormal value representable by the given floating-point DataType \\texttt{T}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "floatmin(T)\n",
       "```\n",
       "\n",
       "The smallest in absolute value non-subnormal value representable by the given floating-point DataType `T`.\n"
      ],
      "text/plain": [
       "\u001b[36m  floatmin(T)\u001b[39m\n",
       "\n",
       "  The smallest in absolute value non-subnormal value representable by the\n",
       "  given floating-point DataType \u001b[36mT\u001b[39m."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?floatmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2250738585072014e-308"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with realmin\n",
    "x = floatmin(Float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0', \"00000000001\", \"0000000000000000000000000000000000000000000000000000\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = bitstring(x)\n",
    "s[1], s[2:12], s[13:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x + 1.0) - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other special floats\n",
    "\n",
    "- `0.0` and `-0.0`: $$e_{10} \\cdots e_0 = 0 \\cdots 0 \\quad \\text{and} \\quad f_1 \\cdots f_{52} = 0 \\cdots 0$$\n",
    "- `Inf` and `-Inf`: $$e_{10} \\cdots e_0 = 1 \\cdots 1 \\quad \\text{and} \\quad f_1 \\cdots f_{52} = 0 \\cdots 0$$\n",
    "- `NaN` (not-a-number): $$e_{10} \\cdots e_0 = 1 \\cdots 1 \\quad \\text{and} \\quad f_1 \\cdots f_{52} \\neq 0$$\n",
    "\n",
    "From [Julia Manual: Mathematical Operations and Elementary Functions](http://julia.readthedocs.org/en/latest/manual/mathematical-operations/):\n",
    "\n",
    "- `Inf` is equal to itself and greater than everything else except `NaN`.\n",
    "- `-Inf` is equal to itself and less then everything else except `NaN`.\n",
    "- `NaN` is not equal to, not less than, and not greater than anything, including itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inf"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with 0.0, -0.0, Inf, -Inf, and NaN\n",
    "\n",
    "1.0/0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "DomainError",
     "evalue": "DomainError with -1.0:\nsqrt will only return a complex result if called with a complex argument. Try sqrt(Complex(x)).",
     "output_type": "error",
     "traceback": [
      "DomainError with -1.0:\nsqrt will only return a complex result if called with a complex argument. Try sqrt(Complex(x)).",
      "",
      "Stacktrace:",
      " [1] throw_complex_domainerror(::Symbol, ::Float64) at .\\math.jl:32",
      " [2] sqrt(::Float64) at .\\math.jl:492",
      " [3] top-level scope at In[10]:1"
     ]
    }
   ],
   "source": [
    "sqrt(-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0 + 1.0im"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(-1.0 + 0.0im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.0 < 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1000000000000000000000000000000000000000000000000000000000000000\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitstring(-0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0000000000000000000000000000000000000000000000000000000000000000\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitstring(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.0 == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.0 === 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaN == NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaN === NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0111111111111000000000000000000000000000000000000000000000000000\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitstring(NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-Inf"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1.0/0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-Inf"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0/-0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0/0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0/Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inf"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inf/0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inf/Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaN + 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine epsilon `eps(Float64)` and the unit roundoff $\\eta$\n",
    "\n",
    "- `1.0 + eps(Float64)` is the **first** `Float64` that is **larger** than `1.0`\n",
    "\n",
    "$$\\mathtt{eps(Float64)} = 2^{-52} \\approx 2.2 \\times 10^{-16}$$\n",
    "\n",
    "- $\\eta = $ `eps(Float64)/2.0` is the largest possible **relative error** due to roundoff\n",
    "\n",
    "$$\\eta = 2^{-53} \\approx 1.1 \\times 10^{-16}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ms\u001b[22m @\u001b[0m\u001b[1me\u001b[22mla\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ms\u001b[22med \u001b[0m\u001b[1me\u001b[22mx\u001b[0m\u001b[1mp\u001b[22mandu\u001b[0m\u001b[1ms\u001b[22mer \u001b[0m\u001b[1me\u001b[22msca\u001b[0m\u001b[1mp\u001b[22me_\u001b[0m\u001b[1ms\u001b[22mtring s\u001b[0m\u001b[1me\u001b[22mt\u001b[0m\u001b[1mp\u001b[22mreci\u001b[0m\u001b[1ms\u001b[22mion p\u001b[0m\u001b[1me\u001b[22makflo\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "eps(::Type{T}) where T<:AbstractFloat\n",
       "eps()\n",
       "\\end{verbatim}\n",
       "Return the \\emph{machine epsilon} of the floating point type \\texttt{T} (\\texttt{T = Float64} by default). This is defined as the gap between 1 and the next largest value representable by \\texttt{typeof(one(T))}, and is equivalent to \\texttt{eps(one(T))}.  (Since \\texttt{eps(T)} is a bound on the \\emph{relative error} of \\texttt{T}, it is a \"dimensionless\" quantity like \\href{@ref}{\\texttt{one}}.)\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> eps()\n",
       "2.220446049250313e-16\n",
       "\n",
       "julia> eps(Float32)\n",
       "1.1920929f-7\n",
       "\n",
       "julia> 1.0 + eps()\n",
       "1.0000000000000002\n",
       "\n",
       "julia> 1.0 + eps()/2\n",
       "1.0\n",
       "\\end{verbatim}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "eps(x::AbstractFloat)\n",
       "\\end{verbatim}\n",
       "Return the \\emph{unit in last place} (ulp) of \\texttt{x}. This is the distance between consecutive representable floating point values at \\texttt{x}. In most cases, if the distance on either side of \\texttt{x} is different, then the larger of the two is taken, that is\n",
       "\n",
       "\\begin{verbatim}\n",
       "eps(x) == max(x-prevfloat(x), nextfloat(x)-x)\n",
       "\\end{verbatim}\n",
       "The exceptions to this rule are the smallest and largest finite values (e.g. \\texttt{nextfloat(-Inf)} and \\texttt{prevfloat(Inf)} for \\href{@ref}{\\texttt{Float64}}), which round to the smaller of the values.\n",
       "\n",
       "The rationale for this behavior is that \\texttt{eps} bounds the floating point rounding error. Under the default \\texttt{RoundNearest} rounding mode, if $y$ is a real number and $x$ is the nearest floating point number to $y$, then\n",
       "\n",
       "$$|y-x| \\leq \\operatorname{eps}(x)/2.$$\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> eps(1.0)\n",
       "2.220446049250313e-16\n",
       "\n",
       "julia> eps(prevfloat(2.0))\n",
       "2.220446049250313e-16\n",
       "\n",
       "julia> eps(2.0)\n",
       "4.440892098500626e-16\n",
       "\n",
       "julia> x = prevfloat(Inf)      # largest finite Float64\n",
       "1.7976931348623157e308\n",
       "\n",
       "julia> x + eps(x)/2            # rounds up\n",
       "Inf\n",
       "\n",
       "julia> x + prevfloat(eps(x)/2) # rounds down\n",
       "1.7976931348623157e308\n",
       "\\end{verbatim}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "eps(::DateTime) -> Millisecond\n",
       "eps(::Date) -> Day\n",
       "eps(::Time) -> Nanosecond\n",
       "\\end{verbatim}\n",
       "Returns \\texttt{Millisecond(1)} for \\texttt{DateTime} values, \\texttt{Day(1)} for \\texttt{Date} values, and \\texttt{Nanosecond(1)} for \\texttt{Time} values.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "eps(::Type{T}) where T<:AbstractFloat\n",
       "eps()\n",
       "```\n",
       "\n",
       "Return the *machine epsilon* of the floating point type `T` (`T = Float64` by default). This is defined as the gap between 1 and the next largest value representable by `typeof(one(T))`, and is equivalent to `eps(one(T))`.  (Since `eps(T)` is a bound on the *relative error* of `T`, it is a \"dimensionless\" quantity like [`one`](@ref).)\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> eps()\n",
       "2.220446049250313e-16\n",
       "\n",
       "julia> eps(Float32)\n",
       "1.1920929f-7\n",
       "\n",
       "julia> 1.0 + eps()\n",
       "1.0000000000000002\n",
       "\n",
       "julia> 1.0 + eps()/2\n",
       "1.0\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "eps(x::AbstractFloat)\n",
       "```\n",
       "\n",
       "Return the *unit in last place* (ulp) of `x`. This is the distance between consecutive representable floating point values at `x`. In most cases, if the distance on either side of `x` is different, then the larger of the two is taken, that is\n",
       "\n",
       "```\n",
       "eps(x) == max(x-prevfloat(x), nextfloat(x)-x)\n",
       "```\n",
       "\n",
       "The exceptions to this rule are the smallest and largest finite values (e.g. `nextfloat(-Inf)` and `prevfloat(Inf)` for [`Float64`](@ref)), which round to the smaller of the values.\n",
       "\n",
       "The rationale for this behavior is that `eps` bounds the floating point rounding error. Under the default `RoundNearest` rounding mode, if $y$ is a real number and $x$ is the nearest floating point number to $y$, then\n",
       "\n",
       "$$\n",
       "|y-x| \\leq \\operatorname{eps}(x)/2.\n",
       "$$\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> eps(1.0)\n",
       "2.220446049250313e-16\n",
       "\n",
       "julia> eps(prevfloat(2.0))\n",
       "2.220446049250313e-16\n",
       "\n",
       "julia> eps(2.0)\n",
       "4.440892098500626e-16\n",
       "\n",
       "julia> x = prevfloat(Inf)      # largest finite Float64\n",
       "1.7976931348623157e308\n",
       "\n",
       "julia> x + eps(x)/2            # rounds up\n",
       "Inf\n",
       "\n",
       "julia> x + prevfloat(eps(x)/2) # rounds down\n",
       "1.7976931348623157e308\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "eps(::DateTime) -> Millisecond\n",
       "eps(::Date) -> Day\n",
       "eps(::Time) -> Nanosecond\n",
       "```\n",
       "\n",
       "Returns `Millisecond(1)` for `DateTime` values, `Day(1)` for `Date` values, and `Nanosecond(1)` for `Time` values.\n"
      ],
      "text/plain": [
       "\u001b[36m  eps(::Type{T}) where T<:AbstractFloat\u001b[39m\n",
       "\u001b[36m  eps()\u001b[39m\n",
       "\n",
       "  Return the \u001b[4mmachine epsilon\u001b[24m of the floating point type \u001b[36mT\u001b[39m (\u001b[36mT = Float64\u001b[39m by\n",
       "  default). This is defined as the gap between 1 and the next largest value\n",
       "  representable by \u001b[36mtypeof(one(T))\u001b[39m, and is equivalent to \u001b[36meps(one(T))\u001b[39m. (Since\n",
       "  \u001b[36meps(T)\u001b[39m is a bound on the \u001b[4mrelative error\u001b[24m of \u001b[36mT\u001b[39m, it is a \"dimensionless\"\n",
       "  quantity like \u001b[36mone\u001b[39m.)\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> eps()\u001b[39m\n",
       "\u001b[36m  2.220446049250313e-16\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> eps(Float32)\u001b[39m\n",
       "\u001b[36m  1.1920929f-7\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> 1.0 + eps()\u001b[39m\n",
       "\u001b[36m  1.0000000000000002\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> 1.0 + eps()/2\u001b[39m\n",
       "\u001b[36m  1.0\u001b[39m\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  eps(x::AbstractFloat)\u001b[39m\n",
       "\n",
       "  Return the \u001b[4munit in last place\u001b[24m (ulp) of \u001b[36mx\u001b[39m. This is the distance between\n",
       "  consecutive representable floating point values at \u001b[36mx\u001b[39m. In most cases, if the\n",
       "  distance on either side of \u001b[36mx\u001b[39m is different, then the larger of the two is\n",
       "  taken, that is\n",
       "\n",
       "\u001b[36m  eps(x) == max(x-prevfloat(x), nextfloat(x)-x)\u001b[39m\n",
       "\n",
       "  The exceptions to this rule are the smallest and largest finite values (e.g.\n",
       "  \u001b[36mnextfloat(-Inf)\u001b[39m and \u001b[36mprevfloat(Inf)\u001b[39m for \u001b[36mFloat64\u001b[39m), which round to the smaller\n",
       "  of the values.\n",
       "\n",
       "  The rationale for this behavior is that \u001b[36meps\u001b[39m bounds the floating point\n",
       "  rounding error. Under the default \u001b[36mRoundNearest\u001b[39m rounding mode, if \u001b[35my\u001b[39m is a real\n",
       "  number and \u001b[35mx\u001b[39m is the nearest floating point number to \u001b[35my\u001b[39m, then\n",
       "\n",
       "\u001b[35m|y-x| \\leq \\operatorname{eps}(x)/2.\u001b[39m\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> eps(1.0)\u001b[39m\n",
       "\u001b[36m  2.220446049250313e-16\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> eps(prevfloat(2.0))\u001b[39m\n",
       "\u001b[36m  2.220446049250313e-16\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> eps(2.0)\u001b[39m\n",
       "\u001b[36m  4.440892098500626e-16\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x = prevfloat(Inf)      # largest finite Float64\u001b[39m\n",
       "\u001b[36m  1.7976931348623157e308\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x + eps(x)/2            # rounds up\u001b[39m\n",
       "\u001b[36m  Inf\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> x + prevfloat(eps(x)/2) # rounds down\u001b[39m\n",
       "\u001b[36m  1.7976931348623157e308\u001b[39m\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  eps(::DateTime) -> Millisecond\u001b[39m\n",
       "\u001b[36m  eps(::Date) -> Day\u001b[39m\n",
       "\u001b[36m  eps(::Time) -> Nanosecond\u001b[39m\n",
       "\n",
       "  Returns \u001b[36mMillisecond(1)\u001b[39m for \u001b[36mDateTime\u001b[39m values, \u001b[36mDay(1)\u001b[39m for \u001b[36mDate\u001b[39m values, and\n",
       "  \u001b[36mNanosecond(1)\u001b[39m for \u001b[36mTime\u001b[39m values."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment with eps\n",
    "ϵ = eps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 + ϵ == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 + ϵ/2.0 == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextfloat(1.0) - 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roundoff error example\n",
    "\n",
    "Suppose we are using a base-10 floating-point system with 4 significant digits, using `RoundNearest`:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\left( 1.112 \\times 10^1 \\right) \\times \\left( 1.112 \\times 10^2 \\right)\n",
    "& = 1.236544 \\times 10^3 \\\\\n",
    "& \\rightarrow 1.237 \\times 10^3 = 1237\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The **absolute error** is $1237 - 1236.544 = 0.456$.\n",
    "\n",
    "The **relative error** is $$\\frac{0.456}{1236.544} \\approx 0.0004 = 0.04 \\%$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 2 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Study the effect of roundoff error by plotting \n",
    "# the absolute error and relative error of converting \n",
    "# Float64 numbers to Float32\n",
    "\n",
    "η = eps(Float32)/2.0\n",
    "\n",
    "x = Array(range(-10.0,stop=10.0,length=10000))\n",
    "y = map(Float32, x)     # Convert x to Float32\n",
    "abserr = broadcast(abs, x-y) # Absolute roundoff errors\n",
    "relerr = abserr./broadcast(abs, x) # Relative roundoff errors\n",
    "\n",
    "using PyPlot\n",
    "\n",
    "subplot(1, 2, 1)\n",
    "plot(x, abserr)\n",
    "ylim(0.0, maximum(abserr))\n",
    "title(\"Absolute error\")\n",
    "\n",
    "subplot(1, 2, 2)\n",
    "plot(x, relerr)\n",
    "# plot(x, η*fill(x), \"r\") creates a red line that should be horizontal, but isn't currently\n",
    "ylim(0.0, maximum(abserr))\n",
    "title(\"Relative error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default rounding mode is `RoundNearest` (round to the nearest floating-point number). This implies that\n",
    "\n",
    "$$ \\frac{|x - \\mathrm{fl}(x)|}{|x|} \\leq \\eta.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roundoff error accumulation\n",
    "\n",
    "When performing arithmetic operations on floats, extra **guard digits** are used to ensure **exact rounding**. This guarantees that the relative error of a floating-point operation (**flop**) is small. More precisely, for floating-point numbers $x$ and $y$, we have\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\mathrm{fl}(x \\pm y) &= (x \\pm y)(1 + \\varepsilon_1) \\\\\n",
    "\\mathrm{fl}(x \\times y) &= (x \\times y)(1 + \\varepsilon_2) \\\\\n",
    "\\mathrm{fl}(x \\div y) &= (x \\div y)(1 + \\varepsilon_3) \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $|\\varepsilon_i| \\leq \\eta$, for $i = 1,2,3$, where $\\eta$ is the unit roundoff.\n",
    "\n",
    "Although the relative error of each flop is small, it is possible to have the roundoff error accumulate and create significant error in the final result. If $E_n$ is the error after $n$ flops, then:\n",
    "\n",
    "- **linear roundoff error accumulation** is when $E_n \\approx c_0 n E_0$\n",
    "- **exponential roundoff error accumulation** is when $E_n \\approx c_1^n E_0$, for some $c_1 > 1$\n",
    "\n",
    "In general, linear roundoff error accumulation is unavoidable. On the other hand, exponential roundoff error accumulation is not acceptable and is an indication of an **unstable algorithm**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General advice\n",
    "\n",
    "1. Adding $x + y$ when $|x| \\gg |y|$ can cause the information in $y$ to be 'lost' in the summation.\n",
    "\n",
    "2. Dividing by very small numbers or multiplying by very large numbers can **magnify error**.\n",
    "\n",
    "3. Subtracting numbers that are almost equal produces **cancellation error**.\n",
    "\n",
    "4. An **overflow** occurs when the result is too large in magnitude to be representable as a float. Result will become either `Inf` or `-Inf`. Overflows should be avoided.\n",
    "\n",
    "4. An **underflow** occurs when the result is too small in magnitude to be representable as a float. Result will become either `0.0` or `-0.0`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example (summation order)\n",
    "\n",
    "This next example shows that summation order can make a difference. We will compute\n",
    "\n",
    "$$\n",
    "s = \\sum_{n = 1}^{1,000,000} \\frac{1}{n}\n",
    "$$\n",
    "\n",
    "in three different ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.997896413852555"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum from largest to smallest\n",
    "s1 = 0.0\n",
    "for n = 1:1e8\n",
    "    s1 += 1.0/n\n",
    "end\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.997896413853447"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum from smallest to largest\n",
    "s2 = 0.0\n",
    "for n = 1e8:-1:1\n",
    "    s2 += 1.0/n\n",
    "end\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.99789641384885"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum largest to smallest and so on\n",
    "s3 = 0.0\n",
    "for n = 1:(1e8)/2\n",
    "    s3 += 1.0/n + 1/(1e8-n+1)\n",
    "end\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.693841433563712e-14"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(s1 - s2)/s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9504687709987854e-13"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(s1 - s3)/s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example (cancellation error)\n",
    "\n",
    "Show that \n",
    "\n",
    "$$\n",
    "\\ln\\left( x - \\sqrt{x^2-1} \\right) = -\\ln\\left( x + \\sqrt{x^2-1} \\right).\n",
    "$$\n",
    "\n",
    "Which formula is more suitable for numerical computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.714973875118524\n",
      "-18.420680743952367\n"
     ]
    }
   ],
   "source": [
    "# Experiment with both formulas\n",
    "x = 1e8/2\n",
    "f1 = log(x - sqrt(x^2 - 1))\n",
    "f2 = -log(x + sqrt(x^2 - 1))\n",
    "\n",
    "println(f1)\n",
    "println(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015725008922262945"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(f1 - f2)/abs(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example (four-digit rounding arithmetic)\n",
    "The quadratic formula to find the roots of \n",
    "### $$x^2+bx+c=0\\qquad \\mbox{are}\\qquad x_1=\\frac{-b+\\sqrt{b^2-4c}}{2} \\quad \\mbox{and} \\quad x_2=\\frac{-b-\\sqrt{b^2-4c}}{2}.$$\n",
    "Using four-digit rounding arithmetic; consider this formula applied to the equation \n",
    "### $$x^2+62.10x+1=0$$ \n",
    "whose roots are\n",
    "### $$x_1=-0.01610723 \\quad \\mbox{and} \\quad x_2=-62.08390.$$ \n",
    "Here, $b$ and $\\sqrt{b^2-4c}$ are nearly equal numbers, since $b^2$ is much larger than $4a$. So,\n",
    "### $$\\sqrt{b^2-4c}=\\sqrt{(62.10)^2-(4.000)(1.000)}=\\sqrt{3856-4.000}=\\sqrt{3852}=62.06$$\n",
    "and \n",
    "### $$\\mathrm{fl}(x_1)=\\frac{-62.10+62.06}{2}=-0.02000,$$\n",
    "a poor approximation of $x_1=-0.01610723$ with **relative error** \n",
    "### $$2.4\\times 10^{-1}.$$\n",
    "To obtain a more accurate four-digit rounding approximation for $x_1$, we change the quadratic formula by rationalizing the numerator:\n",
    "### $$x_1=\\frac{-b+\\sqrt{b^2-4c}}{2}\\left(\\frac{-b-\\sqrt{b^2-4c}}{-b-\\sqrt{b^2-4c}}\\right)=\\frac{-2c}{b+\\sqrt{b^2-4c}}.$$\n",
    "In this case: \n",
    "### $$\\mathrm{fl}(x_1)=\\frac{-2.000}{62.10+62.06}=\\frac{-2.000}{124.2}=-0.01610,$$ \n",
    "which has the small **relative error** \n",
    "### $$6.2\\times 10^{-4}.$$\n",
    "However, this rationalization technique can lead the large **relative error**  \n",
    "### $$1.9\\times 10^{-1}.$$ \n",
    "for $\\mathrm{fl}(x_2)$. Note that the formula \n",
    "### $$x_2=\\frac{-2c}{b-\\sqrt{b^2-4c}} \\qquad \\mbox{and} \\qquad \\mathrm{fl}(x_2)=-50.00.$$\n",
    "The **absolute error** is $12$!!! \n",
    "\n",
    "## $$\n",
    "\\mbox{The lesson: Think before you compute!}\n",
    "$$\n",
    "\n",
    "Accuracy loss due to round-off error can also be reduced by reduce calculations.\n",
    "\n",
    "## Example (evaluate a polynomial): \n",
    "Evaluate $f(x)= x^3 − 6.1x^2 + 3.2x + 1.5$ at $x = 4.71$ using three-digit arithmetic.\n",
    "\n",
    "**Exact:** $$f (4.71) = 104.487111 − 135.32301 + 15.072 + 1.5 = −14.263899.$$\n",
    "\n",
    "**Three-digit (rounding)**: $$f (4.71) = ((105. - 135.) + 15.1) + 1.5 = −13.4.$$\n",
    "\n",
    "An alternative approach is: (**Nested or Ruffini Algorithm**)\n",
    "\n",
    "$$f(x)=x^3 −6.1x^2 +3.2x+1.5=((x−6.1)x+3.2)x+1.5.$$\n",
    "Using **three-digit rounding** arithmetic now produces\n",
    "\n",
    "$$\\begin{align}f (4.71) &= ((4.71 - 6.1)4.71 + 3.2)4.71 + 1.5 \\\\&= ((−1.39)(4.71) + 3.2)4.71 + 1.5 \\\\&= (-6.55 + 3.2)4.71 + 1.5 \\\\&= (-3.35)4.71 + 1.5 \\\\&= −15.8 + 1.5\\\\ &= -14.3.\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
